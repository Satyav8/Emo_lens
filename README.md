
```markdown
# ğŸ­ EmoLens â€” Multimodal Emotion-Aware Learning Assistant  
### _Real-time Emotion Detection â€¢ Adaptive AI Teaching â€¢ Educator Dashboard â€¢ Multimodal Fusion_

EmoLens is an end-to-end **emotion-aware learning system** that analyzes **video**, **audio**, and **text** signals to understand a studentâ€™s emotional state in real time â€” and automatically adapts teaching responses using AI.

This system merges **AI emotional intelligence**, **adaptive learning**, **LLM reasoning**, and **live dashboards** into one cohesive platform.

---

## ğŸš€ Why EmoLens?

Traditional e-learning systems treat every student the same.  
EmoLens changes that.

By understanding how a student **feels** while learning, it can:

- Slow down when the student is frustrated  
- Offer motivation when valence is low  
- Increase difficulty when engagement is high  
- Modify teaching tone dynamically  
- Help educators view emotion timelines  

> ğŸ§  Imagine a tutor that *actually understands* student frustration, confusion, motivation, and focus â€” and reacts instantly.

---

# ğŸŒŸ Key Features

### ğŸ¥ **Video Emotion Recognition (No TensorFlow + CPU friendly)**
Lightweight, heuristic-based face analysis using OpenCV:
- Smile recognition  
- Eye openness  
- Mouth contrast analysis  
- Face-region scoring  
- 7-emotion classification (happy, sad, angry, fear, disgust, surprise, neutral)

### ğŸ”‰ **Audio Emotion Classification**
Extracts MFCCs and predicts expressive states such as:
- Calm / Neutral  
- Happy  
- Sad  
- Angry  
- Fearful  
- Surprised  

### âœï¸ **Text Sentiment & Emotion**
Uses a transformer-based NLP model to classify:
- Anger  
- Sadness  
- Joy  
- Neutral  
- Disgust  
- Surprise  
- Fear  

### ğŸ”— **Multimodal Fusion Engine**
Combines **video + audio + text** into a single **EmotionVector** containing:
- Final emotion  
- Valence (â€“1 to +1)  
- Arousal (0 to 1)  
- Confidence score  
- Per-modality breakdown  

### ğŸ§  **Adaptive Learning Brain**
A lightweight cognitive engine that maps emotions â†’ intelligent micro-actions:
- â€œSlow down and clarifyâ€  
- â€œGive motivational supportâ€  
- â€œIncrease challengeâ€  
- â€œAsk comprehension check questionâ€  

### ğŸ¤– **LLM Teaching Assistant**
Uses an OpenAI model to:
- Answer student questions  
- Adjust tone to student emotion  
- Provide step-by-step explanations  
- Maintain empathy + clarity  

### ğŸ“Š **Educator Dashboard**
View session summaries:
- Emotion timelines  
- Valence and arousal curves  
- Confusion/frustration spikes  
- Exportable session JSON files  
- Per-modality event table  

### ğŸ—‚ï¸ **Supabase Integration**
Stores:
- Timestamped emotional events  
- Fused state  
- Brain recommendations  
- Session IDs  

Securely managed with Streamlit Secrets.

---

# ğŸ§© System Architecture

```

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚     Webcam Input    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Video Emotion Model â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Audio Input   â”‚                    â”‚    Text Input     â”‚
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼                                          â–¼
```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Audio Emotion Model â”‚                       â”‚ Text Emotion Model  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    Multimodal Fusion Engine â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    adaptive_brain (AI)      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   LLM Teaching Response     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Educator Analytics Panel   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

````

---

# ğŸ› ï¸ Tech Stack

### **Frontend**
- Streamlit  
- Custom UI components  
- Emotion Orb visualization  
- Real-time camera + audio input  

### **Backend**
- Python 3.13  
- Custom-built video emotion recognition  
- Audio MFCC + PyTorch classifier  
- Transformer-based text classification  
- Multimodal fusion logic  
- Adaptive Learning Brain  

### **Database**
- Supabase (PostgreSQL + Edge Functions)  

### **AI / NLP**
- OpenAI GPT model  
- Custom tone adaptation  

---

# ğŸ“¦ Installation (Local)

```bash
git clone https://github.com/YOUR_USERNAME/EmoLens.git
cd EmoLens

# create venv
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt
````

Create a `.env`:

```
SUPABASE_URL=...
SUPABASE_KEY=...
OPENAI_API_KEY=...
OPENAI_MODEL=gpt-4o-mini
```

Run app:

```bash
streamlit run app.py
```

---

# â˜ï¸ Deployment (Streamlit Cloud)

1. Push project to GitHub
2. Open [https://share.streamlit.io](https://share.streamlit.io)
3. New App â†’ Select Repo â†’ Choose `app.py`
4. Add Secrets:

```
SUPABASE_URL="..."
SUPABASE_KEY="..."
OPENAI_API_KEY="..."
OPENAI_MODEL="gpt-4o-mini"
```

5. Deploy ğŸ‰

* Multimodal emotion detection
* Adaptive learning responses
* Supabase session tracking
* Dashboard & analytics
* AI conversational layer

### ğŸ”µ In Progress

* Higher-accuracy FER region cropping
* Improved audio emotion stability
* Enhanced UI theme

### ğŸŸ£ Future Enhancements

* YOLO-based face recognition
* Attention tracking (gaze detection)
* Classroom multi-user mode
* Instructor analytics (weekly summaries)
* Personalization model per student

---

# ğŸ’¡ Inspiration

EmoLens was built to explore a future where:

**AI understands not just what students know â€”
but how they feel while learning.**

This project demonstrates the power of combining:

* emotional intelligence
* personalized education
* multimodal AI
* real-time analytics

---

# ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!

If youâ€™d like to add a new modality, improve accuracy, or extend the dashboard â€” feel free to open a PR.

---

# âœ¨ Author

**Satya Prabhas** (2025)
Developer â€¢ Innovator â€¢ AI Enthusiast


